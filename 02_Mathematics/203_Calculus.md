# 203. Calculus for Machine Learning
### (Derivatives, Partial Derivatives, Gradients, Chain Rule, Jacobians, Hessians)

---

## ğŸ§© 1. Introduction

Calculus plays a **critical role** in Machine Learning, especially in **optimization** â€” where we minimize errors and adjust model parameters.

In ML, calculus helps us:
- Optimize cost/loss functions.  
- Understand how models learn through **gradients** and **backpropagation**.  
- Analyze sensitivity and rate of change in parameters.

---

## ğŸ”¹ 2. Basic Concepts

### **2.1 Function**
A **function** maps input(s) to an output:
```text
y = f(x)
```
Example:  
If f(x) = xÂ², then f(3) = 9.

---

### **2.2 Derivative (Single Variable)**

The derivative measures the **rate of change** of a function with respect to its input variable.

Mathematically:
f'(x) = lim(Î”x â†’ 0) [f(x + Î”x) - f(x)] / Î”x

Example:  
If f(x) = xÂ², then f'(x) = 2x

**Interpretation:**  
- The derivative represents the **slope** or gradient at a point.  
- In ML, it tells us how a **change in a parameter** affects the output.

---

## ğŸ”¹ 3. Partial Derivatives

When a function depends on multiple variables:
z = f(x, y)
we compute the rate of change with respect to **one variable at a time**, keeping others constant.

Example:
f(x, y) = xÂ² + 3y  
âˆ‚f/âˆ‚x = 2x,   âˆ‚f/âˆ‚y = 3

**Use in ML:**  
Each weight or bias in a model is a variable. Partial derivatives show how **each parameter** affects the loss.

---

## ğŸ”¹ 4. Gradient

The **gradient** is a **vector** of all partial derivatives of a multivariable function.

âˆ‡f(x, y, z) = [ âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y, âˆ‚f/âˆ‚z ]

It points in the **direction of steepest increase** of the function.

**In ML:**
- We move **opposite to the gradient** (downhill) to minimize loss â€” called **Gradient Descent**.

---

## ğŸ”¹ 5. Chain Rule

Used when functions are **nested** â€” helps compute derivatives of compositions.

dy/dx = (dy/du) * (du/dx)

Example:  
If y = (3x + 1)Â², then dy/dx = 2(3x + 1) * 3 = 6(3x + 1)

**In ML:**  
Used extensively in **backpropagation** â€” propagating gradients backward through network layers.

---

## ğŸ”¹ 6. Jacobian Matrix

The **Jacobian** is a matrix containing all **first-order partial derivatives** of a vector-valued function.

If:  
y = f(x) = [fâ‚(xâ‚, xâ‚‚), fâ‚‚(xâ‚, xâ‚‚)]

Then:  
J = [[âˆ‚fâ‚/âˆ‚xâ‚, âˆ‚fâ‚/âˆ‚xâ‚‚], [âˆ‚fâ‚‚/âˆ‚xâ‚, âˆ‚fâ‚‚/âˆ‚xâ‚‚]]

**In ML:**  
Used in transformations (e.g., data normalization, backpropagation in vector form, and advanced techniques like normalizing flows).

---

## ğŸ”¹ 7. Hessian Matrix

The **Hessian** is a **square matrix of second-order partial derivatives**.

For a function f(x, y):  
H = [[âˆ‚Â²f/âˆ‚xÂ², âˆ‚Â²f/âˆ‚xâˆ‚y], [âˆ‚Â²f/âˆ‚yâˆ‚x, âˆ‚Â²f/âˆ‚yÂ²]]

**In ML:**
- Used in **second-order optimization algorithms** (e.g., Newtonâ€™s Method).  
- Provides curvature information â€” whether the surface is convex or not.

---

## ğŸ”¹ 8. Applications in Machine Learning

| Concept | Application |
|----------|--------------|
| Derivatives | Measure rate of change of loss functions |
| Partial Derivatives | Compute sensitivity of loss wrt. individual parameters |
| Gradients | Used in **Gradient Descent** for optimization |
| Chain Rule | Core of **Backpropagation** in neural networks |
| Jacobian | Describes transformations between variables in vectorized models |
| Hessian | Used in **advanced optimization** (Newton, quasi-Newton methods) |

---

## ğŸ§® 9. Exercises

1. Find the derivative of f(x) = 3xÂ³ + 2xÂ² - 4x + 7  
2. Compute âˆ‚f/âˆ‚x and âˆ‚f/âˆ‚y for f(x, y) = xÂ²y + 3yÂ²  
3. Write the gradient vector for f(x, y, z) = xÂ² + yÂ² + zÂ²  
4. Apply the Chain Rule for f(x) = sin(3xÂ² + 2)  
5. Explain how the Hessian helps in identifying local minima in optimization problems.

---

## ğŸ§¾ Summary

- Calculus helps ML models **learn** by computing **how outputs change** with inputs.  
- **Gradients** and **partial derivatives** drive parameter updates.  
- **Jacobian** and **Hessian** matrices extend these ideas to higher dimensions.  
- Understanding calculus builds intuition for **optimization and backpropagation**.
