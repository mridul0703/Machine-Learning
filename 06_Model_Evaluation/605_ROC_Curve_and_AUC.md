# 605. ROC Curve and AUC (Receiver Operating Characteristic)

---

## ðŸ§© 1. Introduction

The **ROC Curve** and **AUC (Area Under Curve)** are crucial tools for evaluating **classification models**, especially when dealing with **imbalanced datasets**.

While the **Confusion Matrix** shows prediction counts, the ROCâ€“AUC provides a **threshold-independent** view of how well the classifier separates classes.

---

## ðŸ”¹ 2. What is the ROC Curve?

**ROC (Receiver Operating Characteristic)** curve plots:

- **True Positive Rate (TPR)** on the **Y-axis**  
- **False Positive Rate (FPR)** on the **X-axis**

Each point on the curve represents a **different decision threshold** used to classify probabilities into positive/negative classes.

### Formulas:
TPR = TP / (TP + FN)  
FPR = FP / (FP + TN)

| Term | Meaning |
|------|----------|
| **TPR (Recall)** | Fraction of actual positives correctly predicted |
| **FPR** | Fraction of actual negatives incorrectly predicted as positives |

---

## ðŸ”¹ 3. Intuitive Example

| Threshold | TPR (Recall) | FPR |
|------------|---------------|-----|
| 0.9 | 0.60 | 0.05 |
| 0.7 | 0.75 | 0.10 |
| 0.5 | 0.85 | 0.20 |
| 0.3 | 0.95 | 0.40 |

---

## ðŸ”¹ 4. ROC Curve Visualization

```
TPR â†‘
1.0 |                             â€¢ (Ideal model)
    |                           â€¢
    |                        â€¢
    |                     â€¢
0.5 |--------------------â€¢
    |              â€¢
    |         â€¢
    |    â€¢
0.0 |_________________________________________â†’ FPR
     0.0        0.5                1.0
```

---

## ðŸ”¹ 5. Area Under the ROC Curve (AUC)

AUC (Area Under the Curve) measures the **total area** under the ROC curve.

0 â‰¤ AUC â‰¤ 1

| AUC Value | Interpretation |
|------------|----------------|
| 0.9 â€“ 1.0 | Excellent model |
| 0.8 â€“ 0.9 | Good |
| 0.7 â€“ 0.8 | Fair |
| 0.6 â€“ 0.7 | Poor |
| 0.5 | No discrimination (random guessing) |
| < 0.5 | Worse than random |

---

## ðŸ”¹ 6. Why ROCâ€“AUC Matters

âœ… **Threshold-independent** â€“ evaluates all thresholds  
âœ… **Visual trade-off** â€“ between true and false positives  
âœ… **Comparison tool** â€“ compare multiple models on the same plot  
âœ… **Robustness metric** â€“ not affected by class imbalance as much as accuracy  

---

## ðŸ”¹ 7. ROCâ€“AUC vs. Precisionâ€“Recall (PR) Curve

| Aspect | ROC Curve | PR Curve |
|---------|------------|----------|
| Focus | Both positive and negative classes | Positive class only |
| X-axis | False Positive Rate | Recall |
| Useful when | Classes are balanced | Dataset is imbalanced |
| Metric | AUC (Area under ROC) | AUC-PR (Average Precision) |
| Sensitivity to imbalance | Lower | Higher |

---

## ðŸ”¹ 8. ROC Curve in Practice

```python
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_true, y_scores)
auc = roc_auc_score(y_true, y_scores)

plt.plot(fpr, tpr, label=f"AUC = {auc:.2f}")
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()
```

---

## ðŸ”¹ 9. Multi-Class ROCâ€“AUC

```python
from sklearn.metrics import roc_auc_score
roc_auc_score(y_true, y_pred_proba, multi_class='ovr')
```

---

## ðŸ”¹ 10. Example Comparison

| Model | AUC | Comment |
|--------|-----|----------|
| Logistic Regression | 0.87 | Good balance |
| Random Forest | 0.92 | Better separation |
| k-NN | 0.76 | Moderate |
| SVM | 0.90 | Excellent margin classifier |

---

## ðŸ”¹ 11. Interpretation Summary

| Point | Meaning |
|--------|----------|
| (0,0) | Classify everything as negative |
| (1,1) | Classify everything as positive |
| (0,1) | Perfect classifier (ideal) |
| Diagonal line | Random predictions |
| Higher AUC | Better class separability |

---

## ðŸ§© 12. Exercises

1. Compute ROC curve for a binary classifier.  
2. Calculate AUC score using `roc_auc_score()` and interpret it.  
3. Plot ROC curves of multiple models on the same graph.  
4. Compare ROCâ€“AUC vs. PRâ€“AUC for imbalanced data.  
5. Discuss why a model may have high accuracy but low AUC.

---

## ðŸ§¾ 13. Summary

| Concept | Description |
|----------|--------------|
| **ROC Curve** | Shows trade-off between TPR and FPR |
| **AUC** | Area under ROC curve â€“ overall model ability |
| **Good ROC curve** | Close to top-left corner |
| **Use ROCâ€“AUC** | For comparing classifiers |
| **PR Curve** | Better for imbalanced datasets |

---

âœ… **Next Topic:**  
ðŸ“˜ *606. Model Evaluation: Precision, Recall, F1, ROCâ€“AUC Integration*
