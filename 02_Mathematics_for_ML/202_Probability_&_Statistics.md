# 202. Probability & Statistics for Machine Learning  
*(Random Variables, Distributions, Bayes' Theorem, Expectation & Variance)*

---

## ðŸŽ¯ Why Probability & Statistics in ML?

Machine Learning models often **make predictions under uncertainty**.  
Probability & Statistics provide the tools to **quantify uncertainty**, **model randomness**, and **evaluate outcomes**.

- Probability â†’ Theoretical foundation (what *might* happen).  
- Statistics â†’ Practical estimation from data (what *did* happen).  

---

## ðŸŸ¢ 1. Random Variables

A **Random Variable** is a variable whose value is determined by the outcome of a random process.  

### Types:
- **Discrete Random Variable** â†’ Takes specific values (e.g., number of heads in 10 coin flips).  
- **Continuous Random Variable** â†’ Takes values from a continuous range (e.g., height of a person).  

**Notation Example:** 
```yaml
X = number of heads when flipping 3 coins
Possible values: {0, 1, 2, 3}
```
---

## ðŸŸ¡ 2. Probability Distributions

A **probability distribution** describes how the values of a random variable are spread.  

### Discrete Distributions
- **Bernoulli Distribution**:  
  One trial, outcome = {0,1}. Example: coin toss.  
- **Binomial Distribution**:  
  Number of successes in *n* Bernoulli trials. Example: number of heads in 10 flips.  
- **Poisson Distribution**:  
  Number of events in fixed time/space. Example: emails received per hour.  

### Continuous Distributions
- **Uniform Distribution**:  
  All values equally likely. Example: random number from [0,1].  
- **Normal Distribution (Gaussian)**:  
  Bell-shaped curve, most data near mean. Example: heights, test scores.  
- **Exponential Distribution**:  
  Time between events in a Poisson process. Example: time between bus arrivals.  

**Visual Intuition:**  
- Normal â†’ bell curve  
- Uniform â†’ flat line  
- Binomial â†’ histogram of successes  

---

## ðŸ”µ 3. Bayes' Theorem

Bayesâ€™ theorem allows us to **update probabilities** when new information is available.  

**Formula**:
```markdown
P(A|B) = [ P(B|A) * P(A) ] / P(B)
```

Where:  
- `P(A|B)` = Probability of A given B (posterior)  
- `P(B|A)` = Probability of B given A (likelihood)  
- `P(A)` = Prior probability of A  
- `P(B)` = Probability of B (normalizing constant)  

**Example (Medical Diagnosis):**
- 1% of population has a disease â†’ `P(Disease) = 0.01`  
- Test detects disease correctly 99% â†’ `P(Positive|Disease) = 0.99`  
- False positive rate 5% â†’ `P(Positive|NoDisease) = 0.05`  

If a person tests positive, Bayesâ€™ theorem tells us **true chance of having disease** is much lower than 99% (because false positives matter).  

---

## ðŸ”´ 4. Expectation & Variance

### Expectation (Mean)
The **expected value** is the weighted average of all possible values of a random variable.  

**Discrete:**  
```makefile
E[X] = Î£ [ x * P(X=x) ]
```

**Continuous:**
```csharp
E[X] = âˆ« x * f(x) dx
```

### Variance
Variance measures how much values spread out from the mean.  
```shell
Var(X) = E[(X - Î¼)Â²]
= E[XÂ²] - (E[X])Â²
```

### Standard Deviation
```yaml
Ïƒ = sqrt(Var(X))
```

---

## ðŸš€ 5. Applications in Machine Learning

- **Random Variables** â†’ Model uncertainty in outcomes (classification, regression).  
- **Distributions** â†’ Assumptions in algorithms (Naive Bayes, Gaussian Mixture Models).  
- **Bayesâ€™ Theorem** â†’ Probabilistic classifiers (Naive Bayes, Bayesian Networks).  
- **Expectation** â†’ Used in loss functions and decision theory.  
- **Variance** â†’ Measures overfitting & uncertainty of predictions.  

---

## ðŸ“Œ Summary

- Random Variables â†’ represent uncertain quantities  
- Distributions â†’ describe how values occur (Bernoulli, Normal, Poisson, etc.)  
- Bayesâ€™ Theorem â†’ updates beliefs with new evidence  
- Expectation â†’ average outcome  
- Variance â†’ measure of spread/uncertainty  

Probability & Statistics are **the backbone of uncertainty modeling** in ML ðŸš€  

---
