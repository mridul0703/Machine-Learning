# 203. Calculus for Machine Learning  
*(Derivatives, Partial Derivatives, Gradients, Jacobians, Hessians)*

---

## ğŸ¯ Why Calculus in ML?

Calculus is at the **core of Machine Learning optimization**.  
Models â€œlearnâ€ by minimizing a loss function. To do this, we need to understand:

- How a function changes â†’ **Derivatives**  
- How functions behave with many variables â†’ **Partial Derivatives, Gradients**  
- How to optimize efficiently â†’ **Jacobian, Hessian**  

Without calculus, **algorithms like Gradient Descent, Backpropagation, and Optimization** wouldnâ€™t exist.

---

## ğŸŸ¢ 1. Derivatives (Single Variable)

A **derivative** measures how a function changes as its input changes.  

**Definition:**
```
f'(x) = lim(hâ†’0) [ f(x+h) - f(x) ] / h
```

### Intuition:
- Positive derivative â†’ function increasing  
- Negative derivative â†’ function decreasing  
- Zero derivative â†’ possible maximum/minimum  

**Example:**
```
f(x) = xÂ²
f'(x) = 2x
```
- At `x=2`, slope = `4` (increasing fast).  
- At `x=0`, slope = `0` (flat point).  

---

## ğŸŸ¡ 2. Partial Derivatives (Multi-variable)

In ML, most functions depend on **multiple variables**.  
A **partial derivative** is the rate of change of a function with respect to **one variable**, keeping others constant.

**Notation:**
```
âˆ‚f/âˆ‚x â†’ partial derivative of f with respect to x
```

**Example:**
```
f(x, y) = xÂ² + yÂ²
âˆ‚f/âˆ‚x = 2x
âˆ‚f/âˆ‚y = 2y
```

These are the building blocks for **gradients**.

---

## ğŸ”µ 3. Gradient

The **gradient** is a vector of all partial derivatives.  
It points in the direction of the **steepest increase** of a function.

**Definition:**
```
âˆ‡f(x,y) = [ âˆ‚f/âˆ‚x , âˆ‚f/âˆ‚y ]
```

**Example:**
```
f(x, y) = xÂ² + yÂ²
âˆ‡f = [2x, 2y]
```

### Importance in ML:
- Used in **Gradient Descent** to minimize loss functions.  
- Gradient tells us *which direction to move* to reduce error.  

---

## ğŸŸ  4. Jacobian (Vector-Valued Functions)

When we have **multiple outputs** depending on **multiple inputs**, the gradient generalizes to the **Jacobian matrix**.

**Definition:**
```
J = [ âˆ‚f_i / âˆ‚x_j ]
```
where `f` = vector function, `x` = vector of inputs.

**Example:**
```
f(x, y) = [ xÂ² + y ,  yÂ² + x ]
Jacobian J =
[ âˆ‚f1/âˆ‚x   âˆ‚f1/âˆ‚y ]
[ âˆ‚f2/âˆ‚x   âˆ‚f2/âˆ‚y ]

= [ 2x   1 ]
  [ 1   2y ]
```

### ML Applications:
- Used in **Backpropagation** to compute derivatives across neural networks.  
- Helps optimize **vector-valued loss functions**.  

---

## ğŸ”´ 5. Hessian (Second-Order Derivatives)

The **Hessian** is a square matrix of **second-order partial derivatives**.  
It describes the local curvature of a function.  

**Definition:**
```
H = [ âˆ‚Â²f / âˆ‚xiâˆ‚xj ]
```

**Example:**
```
f(x, y) = xÂ² + yÂ²
H =
[ 2   0 ]
[ 0   2 ]
```

### Why it matters:
- If Hessian is **positive definite** â†’ function has a local minimum.  
- If Hessian is **negative definite** â†’ function has a local maximum.  
- Mixed signs â†’ saddle point.  

### ML Applications:
- **Second-order optimization** (Newtonâ€™s method, Quasi-Newton methods).  
- Helps analyze **convexity** of loss functions.  

---

## ğŸš€ 6. Applications in Machine Learning

- **Derivatives** â†’ training linear/logistic regression, SVMs  
- **Partial Derivatives** â†’ handling multi-variable loss functions  
- **Gradients** â†’ Gradient Descent, Backpropagation  
- **Jacobian** â†’ Neural networks with multiple outputs, backprop computations  
- **Hessian** â†’ Advanced optimization, curvature analysis  

---

## ğŸ“Œ Summary

- **Derivatives** â†’ change of function w.r.t one variable  
- **Partial Derivatives** â†’ change in multivariable functions  
- **Gradient** â†’ vector of all partial derivatives (direction of steepest increase)  
- **Jacobian** â†’ matrix for vector-valued functions  
- **Hessian** â†’ curvature information via second derivatives  

Calculus provides the **mathematical machinery that powers optimization in ML** ğŸš€  
